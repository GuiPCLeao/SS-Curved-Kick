{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Neural Network Algorithm to Control a Curved Kicking Mechanism in RoboCup Small Size League**\n",
        "\n",
        "**JINT**\n",
        "\n",
        "Francisco Arthur Bonfim Azevedo - arthurazevedo41@gmail.com\n",
        "\n",
        "Guilherme Pinheiro Cordeiro Le√£o - guipcleao@gmail.com\n",
        "\n",
        "Marcos Ricardo Omena de Albuquerque Maximo"
      ],
      "metadata": {
        "id": "tfLhGAgSfUFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The latest version of this project is available at https://github.com/GuiPCLeao/SS-Curved-Kick"
      ],
      "metadata": {
        "id": "wbCZ4WT57GN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning all saved variables:\n",
        "%reset_selective -f \"^a$\""
      ],
      "metadata": {
        "id": "MpkJhGKQU1pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# download training dataset from google drive:\n",
        "!gdown --id 1SII7pa-RBtZcKlp4ibYhVVheoRGiug6X\n",
        "\n",
        "with zipfile.ZipFile(\"datasets.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall() # extracts in current directory\n",
        "\n",
        "# make results folder to save images\n",
        "if not os.path.exists('results'):\n",
        "  os.makedirs('results')\n",
        "os.remove(\"datasets.zip\")"
      ],
      "metadata": {
        "id": "i9g1fBwj40aW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad5ce08-f8a6-40aa-82ff-a339ee81973c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SII7pa-RBtZcKlp4ibYhVVheoRGiug6X\n",
            "To: /content/datasets.zip\n",
            "100% 14.6M/14.6M [00:00<00:00, 31.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('AVAILABLE DATASET FILES:')\n",
        "data = []\n",
        "for filename in os.listdir('.'):\n",
        "  if filename.endswith(\"csv\"):\n",
        "    # Your code comes here such as\n",
        "    print(f\"- {filename}\")\n",
        "    data.append(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhYb4q2btuGf",
        "outputId": "9ee16f76-243a-4c7c-d3d8-6d0708ad70fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVAILABLE DATASET FILES:\n",
            "- dataset_with_adversaries_tol05.csv\n",
            "- LargeNoCutOutTol06.csv\n",
            "- SmallNoCutOutTol06.csv\n",
            "- LargeNoCutOutTol03.csv\n",
            "- LargeCutOutTol03.csv\n",
            "- TreinaPosicGeneva_tol05.csv\n",
            "- TreinaPosicGeneva_tol05_rev2.csv\n",
            "- SmallCutOutTol06.csv\n",
            "- LargeCutOutTol06.csv\n",
            "- dataset_with_adversaries.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('dataset_with_adversaries_tol05.csv')\n",
        "\n",
        "training_df = df.sample(frac=0.8,random_state=200)\n",
        "validation_df = df.drop(training_df.index)"
      ],
      "metadata": {
        "id": "6zYKp00XcZXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import models, layers, losses, optimizers, activations, metrics, regularizers, utils\n",
        "import os\n",
        "import math\n",
        "\n",
        "class SSAgent:\n",
        "  \"\"\"\n",
        "  Represents a Deep Q-Networks (DQN) SS agent.\n",
        "  \"\"\"\n",
        "  def __init__(self, training_df, validation_df, num_epochs=5000, batch_size=50, lambda_l2=0.0, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Creates a Deep Q-Networks (DQN) SS agent.\n",
        "\n",
        "    :param training_df: dataset used for training the neural network.\n",
        "    :type training_df: pandas.DataFrame.\n",
        "    :param validation_df: dataset used for validating the neural network.\n",
        "    :type validation_df: pandas.DataFrame.\n",
        "    :param num_epochs: number of training epochs.\n",
        "    :type num_epochs: int.\n",
        "    :param batch_size: size of the mini-batches used during training.\n",
        "    :type batch_size: int.\n",
        "    :param lambda_l2: lambda parameter of the L2 regularization.\n",
        "    :type lambda_l2: float.\n",
        "    :param learning_rate: learning rate of the action-value neural network.\n",
        "    :type learning_rate: float.\n",
        "    \"\"\"\n",
        "    self.features_names = [\n",
        "      'Ally_1_row', 'Ally_1_col',\n",
        "      'Ally_2_row', 'Ally_2_col',\n",
        "      'Ally_3_row', 'Ally_3_col',\n",
        "      'Ally_4_row', 'Ally_4_col',\n",
        "      'Ally_5_row', 'Ally_5_col',\n",
        "      'Adv_1_row', 'Adv_1_col',\n",
        "      'Adv_2_row', 'Adv_2_col',\n",
        "      'Adv_3_row', 'Adv_3_col',\n",
        "      'Adv_4_row', 'Adv_4_col',\n",
        "      'Adv_5_row', 'Adv_5_col',\n",
        "      'Adv_6_row', 'Adv_6_col',\n",
        "      'Target_row', 'Target_col',\n",
        "      'ini_row', 'ini_col'\n",
        "    ]\n",
        "    self.output_names = ['V', 'W', 'Theta']\n",
        "    self.lambda_l2 = lambda_l2\n",
        "    self.num_epochs = num_epochs\n",
        "    self.training_input = training_df.loc[:,self.features_names]\n",
        "    self.expected_output = training_df.loc[:,self.output_names]\n",
        "    self.validation_input = validation_df.loc[:,self.features_names]\n",
        "    self.validation_output = validation_df.loc[:,self.output_names]\n",
        "    self.input_size = self.training_input.shape[1]\n",
        "    self.output_size = self.expected_output.shape[1]\n",
        "    self.batch_size = batch_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.model = self._make_model()\n",
        "\n",
        "  def _make_model(self):\n",
        "    \"\"\"\n",
        "    Makes the neural network model using Keras.\n",
        "\n",
        "    :return: sequential neural network.\n",
        "    :rtype: Keras' model.\n",
        "    \"\"\"\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # LAYER 1:\n",
        "    activation_function = layers.LeakyReLU(alpha=0.01)\n",
        "    number_of_neurons = 75\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='1_dense_' + str(number_of_neurons) + '_leaky_relu',\n",
        "                           input_dim=self.input_size,\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    # LAYER 2:\n",
        "    activation_function = layers.LeakyReLU(alpha=0.01)\n",
        "    number_of_neurons = 30\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='2_dense_' + str(number_of_neurons) + '_leaky_relu',\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    # LAYER 3:\n",
        "    activation_function = layers.LeakyReLU(alpha=0.01)\n",
        "    number_of_neurons = 40\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='3_dense_' + str(number_of_neurons) + '_leaky_relu',\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    # LAYER 4:\n",
        "    activation_function = layers.LeakyReLU(alpha=0.01)\n",
        "    number_of_neurons = 50\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='4_dense_' + str(number_of_neurons) + '_leaky_relu',\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    # LAYER 5:\n",
        "    activation_function = layers.LeakyReLU(alpha=0.01)\n",
        "    number_of_neurons = 50\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='5_dense_' + str(number_of_neurons) + '_leaky_relu',\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    # LAYER 6:\n",
        "    activation_function = layers.LeakyReLU(alpha=0.01)\n",
        "    number_of_neurons = 40\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='6_dense_' + str(number_of_neurons) + '_leaky_relu',\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    # LAYER 7:\n",
        "    activation_function = layers.LeakyReLU(alpha=0.01)\n",
        "    number_of_neurons = 40\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='7_dense_' + str(number_of_neurons) + '_leaky_relu',\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    # LAYER 8:\n",
        "    activation_function = layers.LeakyReLU(alpha=0.01)\n",
        "    number_of_neurons = 30\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='8_dense_' + str(number_of_neurons) + '_leaky_relu',\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    # LAYER 9:\n",
        "    activation_function = layers.LeakyReLU(alpha=0.01)\n",
        "    number_of_neurons = 20\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='9_dense_' + str(number_of_neurons) + '_leaky_relu',\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    # LAYER 10:\n",
        "    activation_function = layers.LeakyReLU(alpha=0.01)\n",
        "    number_of_neurons = 20\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='10_dense_' + str(number_of_neurons) + '_leaky_relu',\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    # LAYER 11:\n",
        "    activation_function = layers.LeakyReLU(alpha=0.01)\n",
        "    number_of_neurons = 20\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='11_dense_' + str(number_of_neurons) + '_leaky_relu',\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    # LAYER 12:\n",
        "    activation_function = activations.linear\n",
        "    number_of_neurons = self.output_size\n",
        "    model.add(layers.Dense(number_of_neurons,\n",
        "                           name='12_dense_' + str(number_of_neurons) + '_linear',\n",
        "                           activation=activation_function,\n",
        "                           use_bias=True,\n",
        "                           kernel_regularizer=regularizers.l2(self.lambda_l2)))\n",
        "\n",
        "    model.compile(loss=losses.mse,\n",
        "                  optimizer=optimizers.Adam(learning_rate=self.learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "  def train(self):\n",
        "    \"\"\"\n",
        "    Train the keras model and saves training and validation history.\n",
        "    \"\"\"\n",
        "    self.history = self.model.fit(self.training_input,\n",
        "                                  self.expected_output,\n",
        "                                  validation_data=(self.validation_input,self.validation_output),\n",
        "                                  batch_size=self.batch_size,\n",
        "                                  epochs=self.num_epochs)\n",
        "\n",
        "  def show_results(self):\n",
        "    \"\"\"\n",
        "    Plot the training and validation loss history and saves it to results folder.\n",
        "    \"\"\"\n",
        "    fig_formats = ['png', 'eps']\n",
        "\n",
        "    # Plotting cost function convergence\n",
        "    plt.plot(self.history.history['loss'], label='Train')\n",
        "    plt.plot(self.history.history['val_loss'], label='Validation')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.title('Cost Function Convergence')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    for fig_format in fig_formats:\n",
        "      try:\n",
        "        plt.savefig('results/convergence.' + fig_format, format=fig_format)\n",
        "      except:\n",
        "        print(f\"failed to save figure with extension .{fig_format}\")\n",
        "\n",
        "  def save(self):\n",
        "    \"\"\"\n",
        "    Saves the trained model and model's weights to results folder.\n",
        "    \"\"\"\n",
        "    self.model.save_weights(\"results/weights.h5\")\n",
        "    self.model.save(\"results/model.h5\")\n",
        "    utils.plot_model(self.model,\n",
        "                     to_file='results/model_plot.png',\n",
        "                     show_shapes=True,\n",
        "                     show_layer_names=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "LSHUy4DRenla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment this line to enable training using your GPU\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "\n",
        "num_epochs = 300  # number of epochs for training\n",
        "batch_size = 50\n",
        "#batch_size = int(training_df.shape[0] / 4)\n",
        "lambda_l2 = 0.002 # reduce overfitting\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "ss_agent = SSAgent(training_df, validation_df, num_epochs, batch_size, lambda_l2, learning_rate)\n",
        "ss_agent.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sU5ZCM9ioH2",
        "outputId": "1e988b3b-7b5d-42d7-e13e-4cecf5bdcbd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " 1_dense_75_leaky_relu (Den  (None, 75)                2025      \n",
            " se)                                                             \n",
            "                                                                 \n",
            " 2_dense_30_leaky_relu (Den  (None, 30)                2280      \n",
            " se)                                                             \n",
            "                                                                 \n",
            " 3_dense_40_leaky_relu (Den  (None, 40)                1240      \n",
            " se)                                                             \n",
            "                                                                 \n",
            " 4_dense_50_leaky_relu (Den  (None, 50)                2050      \n",
            " se)                                                             \n",
            "                                                                 \n",
            " 5_dense_50_leaky_relu (Den  (None, 50)                2550      \n",
            " se)                                                             \n",
            "                                                                 \n",
            " 6_dense_40_leaky_relu (Den  (None, 40)                2040      \n",
            " se)                                                             \n",
            "                                                                 \n",
            " 7_dense_40_leaky_relu (Den  (None, 40)                1640      \n",
            " se)                                                             \n",
            "                                                                 \n",
            " 8_dense_30_leaky_relu (Den  (None, 30)                1230      \n",
            " se)                                                             \n",
            "                                                                 \n",
            " 9_dense_20_leaky_relu (Den  (None, 20)                620       \n",
            " se)                                                             \n",
            "                                                                 \n",
            " 10_dense_20_leaky_relu (De  (None, 20)                420       \n",
            " nse)                                                            \n",
            "                                                                 \n",
            " 11_dense_20_leaky_relu (De  (None, 20)                420       \n",
            " nse)                                                            \n",
            "                                                                 \n",
            " 12_dense_3_linear (Dense)   (None, 3)                 63        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16578 (64.76 KB)\n",
            "Trainable params: 16578 (64.76 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "4435/4435 [==============================] - 26s 5ms/step - loss: 5033.4082 - accuracy: 0.9999 - val_loss: 1091.0653 - val_accuracy: 1.0000\n",
            "Epoch 2/300\n",
            "4435/4435 [==============================] - 19s 4ms/step - loss: 815.4637 - accuracy: 1.0000 - val_loss: 296.8735 - val_accuracy: 1.0000\n",
            "Epoch 3/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 204.4318 - accuracy: 1.0000 - val_loss: 129.7166 - val_accuracy: 1.0000\n",
            "Epoch 4/300\n",
            "4435/4435 [==============================] - 20s 5ms/step - loss: 158.8738 - accuracy: 1.0000 - val_loss: 112.2419 - val_accuracy: 1.0000\n",
            "Epoch 5/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 140.0167 - accuracy: 1.0000 - val_loss: 109.8098 - val_accuracy: 1.0000\n",
            "Epoch 6/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 131.3209 - accuracy: 1.0000 - val_loss: 134.4335 - val_accuracy: 1.0000\n",
            "Epoch 7/300\n",
            "4435/4435 [==============================] - 19s 4ms/step - loss: 125.4043 - accuracy: 1.0000 - val_loss: 128.6091 - val_accuracy: 1.0000\n",
            "Epoch 8/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 124.6451 - accuracy: 1.0000 - val_loss: 125.6131 - val_accuracy: 1.0000\n",
            "Epoch 9/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 118.0058 - accuracy: 1.0000 - val_loss: 128.9814 - val_accuracy: 1.0000\n",
            "Epoch 10/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 114.2398 - accuracy: 1.0000 - val_loss: 90.6689 - val_accuracy: 1.0000\n",
            "Epoch 11/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 99.7819 - accuracy: 1.0000 - val_loss: 76.1400 - val_accuracy: 1.0000\n",
            "Epoch 12/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 94.3543 - accuracy: 1.0000 - val_loss: 111.0482 - val_accuracy: 1.0000\n",
            "Epoch 13/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 90.9467 - accuracy: 1.0000 - val_loss: 66.9120 - val_accuracy: 1.0000\n",
            "Epoch 14/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 87.6691 - accuracy: 1.0000 - val_loss: 78.9490 - val_accuracy: 1.0000\n",
            "Epoch 15/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 86.8158 - accuracy: 1.0000 - val_loss: 132.2726 - val_accuracy: 1.0000\n",
            "Epoch 16/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 84.1002 - accuracy: 1.0000 - val_loss: 78.4374 - val_accuracy: 1.0000\n",
            "Epoch 17/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 84.2923 - accuracy: 1.0000 - val_loss: 69.9974 - val_accuracy: 1.0000\n",
            "Epoch 18/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 84.2742 - accuracy: 1.0000 - val_loss: 92.4832 - val_accuracy: 1.0000\n",
            "Epoch 19/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 83.0712 - accuracy: 1.0000 - val_loss: 75.2528 - val_accuracy: 1.0000\n",
            "Epoch 20/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 81.2223 - accuracy: 1.0000 - val_loss: 71.1077 - val_accuracy: 1.0000\n",
            "Epoch 21/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 81.2981 - accuracy: 1.0000 - val_loss: 88.6473 - val_accuracy: 1.0000\n",
            "Epoch 22/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 81.3624 - accuracy: 1.0000 - val_loss: 110.6597 - val_accuracy: 1.0000\n",
            "Epoch 23/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 80.9732 - accuracy: 1.0000 - val_loss: 81.8182 - val_accuracy: 1.0000\n",
            "Epoch 24/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 78.9543 - accuracy: 1.0000 - val_loss: 118.5019 - val_accuracy: 1.0000\n",
            "Epoch 25/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 79.4037 - accuracy: 1.0000 - val_loss: 106.8675 - val_accuracy: 1.0000\n",
            "Epoch 26/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 78.5534 - accuracy: 1.0000 - val_loss: 77.3403 - val_accuracy: 1.0000\n",
            "Epoch 27/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 77.8984 - accuracy: 1.0000 - val_loss: 66.3580 - val_accuracy: 1.0000\n",
            "Epoch 28/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 76.7073 - accuracy: 1.0000 - val_loss: 67.9356 - val_accuracy: 1.0000\n",
            "Epoch 29/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 77.7885 - accuracy: 1.0000 - val_loss: 134.4117 - val_accuracy: 1.0000\n",
            "Epoch 30/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 78.2126 - accuracy: 1.0000 - val_loss: 75.6092 - val_accuracy: 1.0000\n",
            "Epoch 31/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 77.4355 - accuracy: 1.0000 - val_loss: 63.5824 - val_accuracy: 1.0000\n",
            "Epoch 32/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 76.7722 - accuracy: 1.0000 - val_loss: 85.5518 - val_accuracy: 1.0000\n",
            "Epoch 33/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 76.0390 - accuracy: 1.0000 - val_loss: 72.4712 - val_accuracy: 1.0000\n",
            "Epoch 34/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 75.4263 - accuracy: 1.0000 - val_loss: 113.4999 - val_accuracy: 1.0000\n",
            "Epoch 35/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 76.4968 - accuracy: 1.0000 - val_loss: 67.9277 - val_accuracy: 1.0000\n",
            "Epoch 36/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 75.7245 - accuracy: 1.0000 - val_loss: 69.8038 - val_accuracy: 1.0000\n",
            "Epoch 37/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 74.6174 - accuracy: 1.0000 - val_loss: 72.8623 - val_accuracy: 1.0000\n",
            "Epoch 38/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 74.8154 - accuracy: 1.0000 - val_loss: 61.1502 - val_accuracy: 1.0000\n",
            "Epoch 39/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 74.3118 - accuracy: 1.0000 - val_loss: 58.0613 - val_accuracy: 1.0000\n",
            "Epoch 40/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 73.7326 - accuracy: 1.0000 - val_loss: 73.9433 - val_accuracy: 1.0000\n",
            "Epoch 41/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 73.3243 - accuracy: 1.0000 - val_loss: 67.9438 - val_accuracy: 1.0000\n",
            "Epoch 42/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 72.8905 - accuracy: 1.0000 - val_loss: 61.4025 - val_accuracy: 1.0000\n",
            "Epoch 43/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 73.1058 - accuracy: 1.0000 - val_loss: 76.1284 - val_accuracy: 1.0000\n",
            "Epoch 44/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 72.8249 - accuracy: 1.0000 - val_loss: 62.0192 - val_accuracy: 1.0000\n",
            "Epoch 45/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 72.7492 - accuracy: 1.0000 - val_loss: 65.6393 - val_accuracy: 1.0000\n",
            "Epoch 46/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 73.3094 - accuracy: 1.0000 - val_loss: 86.6818 - val_accuracy: 1.0000\n",
            "Epoch 47/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 72.2574 - accuracy: 1.0000 - val_loss: 63.4688 - val_accuracy: 1.0000\n",
            "Epoch 48/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 71.6322 - accuracy: 1.0000 - val_loss: 90.9561 - val_accuracy: 1.0000\n",
            "Epoch 49/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 71.0810 - accuracy: 1.0000 - val_loss: 71.2974 - val_accuracy: 1.0000\n",
            "Epoch 50/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 70.2709 - accuracy: 1.0000 - val_loss: 71.5376 - val_accuracy: 1.0000\n",
            "Epoch 51/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 70.8829 - accuracy: 1.0000 - val_loss: 75.2277 - val_accuracy: 1.0000\n",
            "Epoch 52/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 71.1640 - accuracy: 1.0000 - val_loss: 66.7986 - val_accuracy: 1.0000\n",
            "Epoch 53/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 70.2020 - accuracy: 1.0000 - val_loss: 63.0228 - val_accuracy: 1.0000\n",
            "Epoch 54/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 71.2940 - accuracy: 1.0000 - val_loss: 62.4663 - val_accuracy: 1.0000\n",
            "Epoch 55/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 70.6885 - accuracy: 1.0000 - val_loss: 72.6964 - val_accuracy: 1.0000\n",
            "Epoch 56/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 71.5585 - accuracy: 1.0000 - val_loss: 68.4135 - val_accuracy: 1.0000\n",
            "Epoch 57/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 70.4304 - accuracy: 1.0000 - val_loss: 66.5399 - val_accuracy: 1.0000\n",
            "Epoch 58/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 69.5729 - accuracy: 1.0000 - val_loss: 85.9156 - val_accuracy: 1.0000\n",
            "Epoch 59/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 70.4104 - accuracy: 1.0000 - val_loss: 56.2850 - val_accuracy: 1.0000\n",
            "Epoch 60/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 69.1228 - accuracy: 1.0000 - val_loss: 67.7904 - val_accuracy: 1.0000\n",
            "Epoch 61/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 69.1922 - accuracy: 1.0000 - val_loss: 60.7062 - val_accuracy: 1.0000\n",
            "Epoch 62/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 68.8269 - accuracy: 1.0000 - val_loss: 59.3637 - val_accuracy: 1.0000\n",
            "Epoch 63/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 69.3682 - accuracy: 1.0000 - val_loss: 84.7130 - val_accuracy: 1.0000\n",
            "Epoch 64/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 69.4144 - accuracy: 1.0000 - val_loss: 68.3758 - val_accuracy: 1.0000\n",
            "Epoch 65/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 68.0420 - accuracy: 1.0000 - val_loss: 67.5682 - val_accuracy: 1.0000\n",
            "Epoch 66/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 69.3201 - accuracy: 1.0000 - val_loss: 60.0873 - val_accuracy: 1.0000\n",
            "Epoch 67/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 68.9090 - accuracy: 1.0000 - val_loss: 59.2667 - val_accuracy: 1.0000\n",
            "Epoch 68/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 68.4001 - accuracy: 1.0000 - val_loss: 63.1302 - val_accuracy: 1.0000\n",
            "Epoch 69/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 67.8080 - accuracy: 1.0000 - val_loss: 74.9148 - val_accuracy: 1.0000\n",
            "Epoch 70/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 68.5483 - accuracy: 1.0000 - val_loss: 59.7859 - val_accuracy: 1.0000\n",
            "Epoch 71/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 67.1286 - accuracy: 1.0000 - val_loss: 85.9736 - val_accuracy: 1.0000\n",
            "Epoch 72/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 67.4458 - accuracy: 1.0000 - val_loss: 69.3639 - val_accuracy: 1.0000\n",
            "Epoch 73/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 68.3514 - accuracy: 1.0000 - val_loss: 70.3665 - val_accuracy: 1.0000\n",
            "Epoch 74/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 68.4253 - accuracy: 1.0000 - val_loss: 96.7182 - val_accuracy: 1.0000\n",
            "Epoch 75/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 67.5513 - accuracy: 1.0000 - val_loss: 62.1583 - val_accuracy: 1.0000\n",
            "Epoch 76/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 67.8914 - accuracy: 1.0000 - val_loss: 83.6754 - val_accuracy: 1.0000\n",
            "Epoch 77/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 67.3636 - accuracy: 1.0000 - val_loss: 57.3719 - val_accuracy: 1.0000\n",
            "Epoch 78/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 67.1401 - accuracy: 1.0000 - val_loss: 55.6357 - val_accuracy: 1.0000\n",
            "Epoch 79/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 67.5135 - accuracy: 1.0000 - val_loss: 65.8879 - val_accuracy: 1.0000\n",
            "Epoch 80/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 66.8975 - accuracy: 1.0000 - val_loss: 56.0837 - val_accuracy: 1.0000\n",
            "Epoch 81/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 67.3913 - accuracy: 1.0000 - val_loss: 64.9343 - val_accuracy: 1.0000\n",
            "Epoch 82/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 66.4228 - accuracy: 1.0000 - val_loss: 60.3534 - val_accuracy: 1.0000\n",
            "Epoch 83/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 67.1359 - accuracy: 1.0000 - val_loss: 91.8541 - val_accuracy: 1.0000\n",
            "Epoch 84/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 67.1188 - accuracy: 1.0000 - val_loss: 87.3068 - val_accuracy: 1.0000\n",
            "Epoch 85/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 66.5906 - accuracy: 1.0000 - val_loss: 58.5856 - val_accuracy: 1.0000\n",
            "Epoch 86/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 66.1853 - accuracy: 1.0000 - val_loss: 76.5766 - val_accuracy: 1.0000\n",
            "Epoch 87/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 66.9403 - accuracy: 1.0000 - val_loss: 73.0613 - val_accuracy: 1.0000\n",
            "Epoch 88/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 66.9703 - accuracy: 1.0000 - val_loss: 69.8658 - val_accuracy: 1.0000\n",
            "Epoch 89/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 65.9887 - accuracy: 1.0000 - val_loss: 61.9529 - val_accuracy: 1.0000\n",
            "Epoch 90/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 66.9032 - accuracy: 1.0000 - val_loss: 58.4532 - val_accuracy: 1.0000\n",
            "Epoch 91/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 66.2348 - accuracy: 1.0000 - val_loss: 75.3408 - val_accuracy: 1.0000\n",
            "Epoch 92/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 66.3152 - accuracy: 1.0000 - val_loss: 67.8626 - val_accuracy: 1.0000\n",
            "Epoch 93/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 65.8404 - accuracy: 1.0000 - val_loss: 57.5181 - val_accuracy: 1.0000\n",
            "Epoch 94/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 66.6012 - accuracy: 1.0000 - val_loss: 56.7649 - val_accuracy: 1.0000\n",
            "Epoch 95/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 66.4410 - accuracy: 1.0000 - val_loss: 58.3846 - val_accuracy: 1.0000\n",
            "Epoch 96/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 65.9229 - accuracy: 1.0000 - val_loss: 65.9461 - val_accuracy: 1.0000\n",
            "Epoch 97/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 66.4304 - accuracy: 1.0000 - val_loss: 62.3775 - val_accuracy: 1.0000\n",
            "Epoch 98/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 66.1013 - accuracy: 1.0000 - val_loss: 59.1911 - val_accuracy: 1.0000\n",
            "Epoch 99/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 65.8767 - accuracy: 1.0000 - val_loss: 57.1208 - val_accuracy: 1.0000\n",
            "Epoch 100/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 66.4312 - accuracy: 1.0000 - val_loss: 76.5851 - val_accuracy: 1.0000\n",
            "Epoch 101/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 65.9193 - accuracy: 1.0000 - val_loss: 65.4615 - val_accuracy: 1.0000\n",
            "Epoch 102/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 65.4594 - accuracy: 1.0000 - val_loss: 55.6008 - val_accuracy: 1.0000\n",
            "Epoch 103/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 66.0969 - accuracy: 1.0000 - val_loss: 65.9545 - val_accuracy: 1.0000\n",
            "Epoch 104/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 65.7330 - accuracy: 1.0000 - val_loss: 73.4292 - val_accuracy: 1.0000\n",
            "Epoch 105/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 66.0310 - accuracy: 1.0000 - val_loss: 93.1543 - val_accuracy: 1.0000\n",
            "Epoch 106/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 65.2516 - accuracy: 1.0000 - val_loss: 61.1640 - val_accuracy: 1.0000\n",
            "Epoch 107/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 66.0177 - accuracy: 1.0000 - val_loss: 61.1191 - val_accuracy: 1.0000\n",
            "Epoch 108/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 65.1474 - accuracy: 1.0000 - val_loss: 57.2169 - val_accuracy: 1.0000\n",
            "Epoch 109/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 64.8468 - accuracy: 1.0000 - val_loss: 62.8417 - val_accuracy: 1.0000\n",
            "Epoch 110/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 65.4061 - accuracy: 1.0000 - val_loss: 64.5692 - val_accuracy: 1.0000\n",
            "Epoch 111/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 65.6037 - accuracy: 1.0000 - val_loss: 107.7965 - val_accuracy: 1.0000\n",
            "Epoch 112/300\n",
            "4435/4435 [==============================] - 16s 3ms/step - loss: 64.9932 - accuracy: 1.0000 - val_loss: 58.9757 - val_accuracy: 1.0000\n",
            "Epoch 113/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 65.6356 - accuracy: 1.0000 - val_loss: 71.6391 - val_accuracy: 1.0000\n",
            "Epoch 114/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 65.3506 - accuracy: 1.0000 - val_loss: 57.6214 - val_accuracy: 1.0000\n",
            "Epoch 115/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 65.3692 - accuracy: 1.0000 - val_loss: 57.8996 - val_accuracy: 1.0000\n",
            "Epoch 116/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 64.7340 - accuracy: 1.0000 - val_loss: 59.7481 - val_accuracy: 1.0000\n",
            "Epoch 117/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 64.8571 - accuracy: 1.0000 - val_loss: 55.6439 - val_accuracy: 1.0000\n",
            "Epoch 118/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 64.9169 - accuracy: 1.0000 - val_loss: 57.8922 - val_accuracy: 1.0000\n",
            "Epoch 119/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 64.6207 - accuracy: 1.0000 - val_loss: 80.9816 - val_accuracy: 1.0000\n",
            "Epoch 120/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 64.6944 - accuracy: 1.0000 - val_loss: 60.2465 - val_accuracy: 1.0000\n",
            "Epoch 121/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 64.9237 - accuracy: 1.0000 - val_loss: 61.6528 - val_accuracy: 1.0000\n",
            "Epoch 122/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 64.7065 - accuracy: 1.0000 - val_loss: 62.7219 - val_accuracy: 1.0000\n",
            "Epoch 123/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 64.7342 - accuracy: 1.0000 - val_loss: 60.2981 - val_accuracy: 1.0000\n",
            "Epoch 124/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 64.4281 - accuracy: 1.0000 - val_loss: 61.0394 - val_accuracy: 1.0000\n",
            "Epoch 125/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 64.3718 - accuracy: 1.0000 - val_loss: 63.3383 - val_accuracy: 1.0000\n",
            "Epoch 126/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 65.1800 - accuracy: 1.0000 - val_loss: 58.5008 - val_accuracy: 1.0000\n",
            "Epoch 127/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 64.7239 - accuracy: 1.0000 - val_loss: 73.3798 - val_accuracy: 1.0000\n",
            "Epoch 128/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 64.3490 - accuracy: 1.0000 - val_loss: 61.6825 - val_accuracy: 1.0000\n",
            "Epoch 129/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 63.8391 - accuracy: 1.0000 - val_loss: 83.3980 - val_accuracy: 1.0000\n",
            "Epoch 130/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 64.9952 - accuracy: 1.0000 - val_loss: 57.5646 - val_accuracy: 1.0000\n",
            "Epoch 131/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 64.3513 - accuracy: 1.0000 - val_loss: 72.1775 - val_accuracy: 1.0000\n",
            "Epoch 132/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 64.8879 - accuracy: 1.0000 - val_loss: 78.0235 - val_accuracy: 1.0000\n",
            "Epoch 133/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.7392 - accuracy: 1.0000 - val_loss: 100.0328 - val_accuracy: 1.0000\n",
            "Epoch 134/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 65.0023 - accuracy: 1.0000 - val_loss: 83.3125 - val_accuracy: 1.0000\n",
            "Epoch 135/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 64.1724 - accuracy: 1.0000 - val_loss: 62.3126 - val_accuracy: 1.0000\n",
            "Epoch 136/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.5947 - accuracy: 1.0000 - val_loss: 100.2831 - val_accuracy: 1.0000\n",
            "Epoch 137/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 64.2431 - accuracy: 1.0000 - val_loss: 68.8399 - val_accuracy: 1.0000\n",
            "Epoch 138/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.9833 - accuracy: 1.0000 - val_loss: 60.6134 - val_accuracy: 1.0000\n",
            "Epoch 139/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 64.3717 - accuracy: 1.0000 - val_loss: 58.6189 - val_accuracy: 1.0000\n",
            "Epoch 140/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.7473 - accuracy: 1.0000 - val_loss: 56.1983 - val_accuracy: 1.0000\n",
            "Epoch 141/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.4816 - accuracy: 1.0000 - val_loss: 62.4028 - val_accuracy: 1.0000\n",
            "Epoch 142/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 63.0851 - accuracy: 1.0000 - val_loss: 68.5475 - val_accuracy: 1.0000\n",
            "Epoch 143/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.7471 - accuracy: 1.0000 - val_loss: 57.5308 - val_accuracy: 1.0000\n",
            "Epoch 144/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 63.2296 - accuracy: 1.0000 - val_loss: 73.9749 - val_accuracy: 1.0000\n",
            "Epoch 145/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 63.8604 - accuracy: 1.0000 - val_loss: 57.8945 - val_accuracy: 1.0000\n",
            "Epoch 146/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.9856 - accuracy: 1.0000 - val_loss: 56.2288 - val_accuracy: 1.0000\n",
            "Epoch 147/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.3807 - accuracy: 1.0000 - val_loss: 67.3404 - val_accuracy: 1.0000\n",
            "Epoch 148/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 64.6202 - accuracy: 1.0000 - val_loss: 55.8856 - val_accuracy: 1.0000\n",
            "Epoch 149/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.7270 - accuracy: 1.0000 - val_loss: 75.1492 - val_accuracy: 1.0000\n",
            "Epoch 150/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 63.8314 - accuracy: 1.0000 - val_loss: 61.9810 - val_accuracy: 1.0000\n",
            "Epoch 151/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 63.7779 - accuracy: 1.0000 - val_loss: 64.0869 - val_accuracy: 1.0000\n",
            "Epoch 152/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.2130 - accuracy: 1.0000 - val_loss: 69.1753 - val_accuracy: 1.0000\n",
            "Epoch 153/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.7517 - accuracy: 1.0000 - val_loss: 85.4190 - val_accuracy: 1.0000\n",
            "Epoch 154/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.2649 - accuracy: 1.0000 - val_loss: 58.4377 - val_accuracy: 1.0000\n",
            "Epoch 155/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.3457 - accuracy: 1.0000 - val_loss: 116.7410 - val_accuracy: 1.0000\n",
            "Epoch 156/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.8238 - accuracy: 1.0000 - val_loss: 62.3731 - val_accuracy: 1.0000\n",
            "Epoch 157/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.1502 - accuracy: 1.0000 - val_loss: 60.8381 - val_accuracy: 1.0000\n",
            "Epoch 158/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.0460 - accuracy: 1.0000 - val_loss: 59.9911 - val_accuracy: 1.0000\n",
            "Epoch 159/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 63.3543 - accuracy: 1.0000 - val_loss: 57.6167 - val_accuracy: 1.0000\n",
            "Epoch 160/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.5747 - accuracy: 1.0000 - val_loss: 59.2085 - val_accuracy: 1.0000\n",
            "Epoch 161/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 63.6897 - accuracy: 1.0000 - val_loss: 54.9893 - val_accuracy: 1.0000\n",
            "Epoch 162/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 63.1785 - accuracy: 1.0000 - val_loss: 88.3690 - val_accuracy: 1.0000\n",
            "Epoch 163/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.9402 - accuracy: 1.0000 - val_loss: 57.3662 - val_accuracy: 1.0000\n",
            "Epoch 164/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 63.3466 - accuracy: 1.0000 - val_loss: 76.7549 - val_accuracy: 1.0000\n",
            "Epoch 165/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.9790 - accuracy: 1.0000 - val_loss: 56.4395 - val_accuracy: 1.0000\n",
            "Epoch 166/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.8993 - accuracy: 1.0000 - val_loss: 58.3152 - val_accuracy: 1.0000\n",
            "Epoch 167/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.7793 - accuracy: 1.0000 - val_loss: 79.0731 - val_accuracy: 1.0000\n",
            "Epoch 168/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 63.7365 - accuracy: 1.0000 - val_loss: 62.6233 - val_accuracy: 1.0000\n",
            "Epoch 169/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.9732 - accuracy: 1.0000 - val_loss: 57.2805 - val_accuracy: 1.0000\n",
            "Epoch 170/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 63.7003 - accuracy: 1.0000 - val_loss: 61.8130 - val_accuracy: 1.0000\n",
            "Epoch 171/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.8506 - accuracy: 1.0000 - val_loss: 61.2408 - val_accuracy: 1.0000\n",
            "Epoch 172/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.9372 - accuracy: 1.0000 - val_loss: 55.6970 - val_accuracy: 1.0000\n",
            "Epoch 173/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 63.1398 - accuracy: 1.0000 - val_loss: 56.2575 - val_accuracy: 1.0000\n",
            "Epoch 174/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 62.9928 - accuracy: 1.0000 - val_loss: 55.2456 - val_accuracy: 1.0000\n",
            "Epoch 175/300\n",
            "4435/4435 [==============================] - 19s 4ms/step - loss: 63.1396 - accuracy: 1.0000 - val_loss: 64.8311 - val_accuracy: 1.0000\n",
            "Epoch 176/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.8750 - accuracy: 1.0000 - val_loss: 58.4062 - val_accuracy: 1.0000\n",
            "Epoch 177/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 62.8499 - accuracy: 1.0000 - val_loss: 60.4292 - val_accuracy: 1.0000\n",
            "Epoch 178/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 62.2569 - accuracy: 1.0000 - val_loss: 59.2888 - val_accuracy: 1.0000\n",
            "Epoch 179/300\n",
            "4435/4435 [==============================] - 14s 3ms/step - loss: 62.4337 - accuracy: 1.0000 - val_loss: 61.1357 - val_accuracy: 1.0000\n",
            "Epoch 180/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 63.2452 - accuracy: 1.0000 - val_loss: 59.2695 - val_accuracy: 1.0000\n",
            "Epoch 181/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.8018 - accuracy: 1.0000 - val_loss: 68.4753 - val_accuracy: 1.0000\n",
            "Epoch 182/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.9734 - accuracy: 1.0000 - val_loss: 63.7703 - val_accuracy: 1.0000\n",
            "Epoch 183/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.0657 - accuracy: 1.0000 - val_loss: 58.4205 - val_accuracy: 1.0000\n",
            "Epoch 184/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 62.8837 - accuracy: 1.0000 - val_loss: 57.4925 - val_accuracy: 1.0000\n",
            "Epoch 185/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 62.0270 - accuracy: 1.0000 - val_loss: 61.2467 - val_accuracy: 1.0000\n",
            "Epoch 186/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.5996 - accuracy: 1.0000 - val_loss: 58.0037 - val_accuracy: 1.0000\n",
            "Epoch 187/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.3035 - accuracy: 1.0000 - val_loss: 60.6738 - val_accuracy: 1.0000\n",
            "Epoch 188/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 62.8781 - accuracy: 1.0000 - val_loss: 68.4491 - val_accuracy: 1.0000\n",
            "Epoch 189/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 62.4834 - accuracy: 1.0000 - val_loss: 55.7213 - val_accuracy: 1.0000\n",
            "Epoch 190/300\n",
            "4435/4435 [==============================] - 20s 4ms/step - loss: 62.5280 - accuracy: 1.0000 - val_loss: 55.8305 - val_accuracy: 1.0000\n",
            "Epoch 191/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.8555 - accuracy: 1.0000 - val_loss: 60.2732 - val_accuracy: 1.0000\n",
            "Epoch 192/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.5997 - accuracy: 1.0000 - val_loss: 63.2692 - val_accuracy: 1.0000\n",
            "Epoch 193/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.5448 - accuracy: 1.0000 - val_loss: 102.8089 - val_accuracy: 1.0000\n",
            "Epoch 194/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.1750 - accuracy: 1.0000 - val_loss: 61.8571 - val_accuracy: 1.0000\n",
            "Epoch 195/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.3265 - accuracy: 1.0000 - val_loss: 64.3428 - val_accuracy: 1.0000\n",
            "Epoch 196/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.2959 - accuracy: 1.0000 - val_loss: 63.2813 - val_accuracy: 1.0000\n",
            "Epoch 197/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.3223 - accuracy: 1.0000 - val_loss: 65.7703 - val_accuracy: 1.0000\n",
            "Epoch 198/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.4310 - accuracy: 1.0000 - val_loss: 66.3924 - val_accuracy: 1.0000\n",
            "Epoch 199/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.3307 - accuracy: 1.0000 - val_loss: 57.6120 - val_accuracy: 1.0000\n",
            "Epoch 200/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.1754 - accuracy: 1.0000 - val_loss: 70.5263 - val_accuracy: 1.0000\n",
            "Epoch 201/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 62.2208 - accuracy: 1.0000 - val_loss: 64.2167 - val_accuracy: 1.0000\n",
            "Epoch 202/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.2395 - accuracy: 1.0000 - val_loss: 71.4442 - val_accuracy: 1.0000\n",
            "Epoch 203/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 62.1133 - accuracy: 1.0000 - val_loss: 57.0253 - val_accuracy: 1.0000\n",
            "Epoch 204/300\n",
            "4435/4435 [==============================] - 19s 4ms/step - loss: 61.4364 - accuracy: 1.0000 - val_loss: 67.2063 - val_accuracy: 1.0000\n",
            "Epoch 205/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 62.7761 - accuracy: 1.0000 - val_loss: 65.9473 - val_accuracy: 1.0000\n",
            "Epoch 206/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.9568 - accuracy: 1.0000 - val_loss: 64.1359 - val_accuracy: 1.0000\n",
            "Epoch 207/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.7163 - accuracy: 1.0000 - val_loss: 55.4696 - val_accuracy: 1.0000\n",
            "Epoch 208/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.9839 - accuracy: 1.0000 - val_loss: 58.8728 - val_accuracy: 1.0000\n",
            "Epoch 209/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.9679 - accuracy: 1.0000 - val_loss: 82.7979 - val_accuracy: 1.0000\n",
            "Epoch 210/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 62.2778 - accuracy: 1.0000 - val_loss: 62.5671 - val_accuracy: 1.0000\n",
            "Epoch 211/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.3521 - accuracy: 1.0000 - val_loss: 56.0591 - val_accuracy: 1.0000\n",
            "Epoch 212/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 62.5498 - accuracy: 1.0000 - val_loss: 57.1005 - val_accuracy: 1.0000\n",
            "Epoch 213/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 61.5916 - accuracy: 1.0000 - val_loss: 55.6519 - val_accuracy: 1.0000\n",
            "Epoch 214/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 61.8572 - accuracy: 1.0000 - val_loss: 63.3011 - val_accuracy: 1.0000\n",
            "Epoch 215/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.9522 - accuracy: 1.0000 - val_loss: 56.1275 - val_accuracy: 1.0000\n",
            "Epoch 216/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.6700 - accuracy: 1.0000 - val_loss: 58.1888 - val_accuracy: 1.0000\n",
            "Epoch 217/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 61.7033 - accuracy: 1.0000 - val_loss: 54.6473 - val_accuracy: 1.0000\n",
            "Epoch 218/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.0350 - accuracy: 1.0000 - val_loss: 57.3403 - val_accuracy: 1.0000\n",
            "Epoch 219/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 61.6423 - accuracy: 1.0000 - val_loss: 61.6552 - val_accuracy: 1.0000\n",
            "Epoch 220/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 61.7543 - accuracy: 1.0000 - val_loss: 70.8417 - val_accuracy: 1.0000\n",
            "Epoch 221/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 61.5697 - accuracy: 1.0000 - val_loss: 60.6610 - val_accuracy: 1.0000\n",
            "Epoch 222/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 61.5341 - accuracy: 1.0000 - val_loss: 71.3453 - val_accuracy: 1.0000\n",
            "Epoch 223/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 62.7896 - accuracy: 1.0000 - val_loss: 59.2275 - val_accuracy: 1.0000\n",
            "Epoch 224/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.7517 - accuracy: 1.0000 - val_loss: 56.3795 - val_accuracy: 1.0000\n",
            "Epoch 225/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 61.5279 - accuracy: 1.0000 - val_loss: 68.5365 - val_accuracy: 1.0000\n",
            "Epoch 226/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.7769 - accuracy: 1.0000 - val_loss: 66.4200 - val_accuracy: 1.0000\n",
            "Epoch 227/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.4080 - accuracy: 1.0000 - val_loss: 61.1565 - val_accuracy: 1.0000\n",
            "Epoch 228/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 61.3541 - accuracy: 1.0000 - val_loss: 57.6703 - val_accuracy: 1.0000\n",
            "Epoch 229/300\n",
            "4435/4435 [==============================] - 19s 4ms/step - loss: 62.1056 - accuracy: 1.0000 - val_loss: 68.2242 - val_accuracy: 1.0000\n",
            "Epoch 230/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.4806 - accuracy: 1.0000 - val_loss: 58.3250 - val_accuracy: 1.0000\n",
            "Epoch 231/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 62.1750 - accuracy: 1.0000 - val_loss: 64.0389 - val_accuracy: 1.0000\n",
            "Epoch 232/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 62.2641 - accuracy: 1.0000 - val_loss: 62.0261 - val_accuracy: 1.0000\n",
            "Epoch 233/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 61.5910 - accuracy: 1.0000 - val_loss: 62.4855 - val_accuracy: 1.0000\n",
            "Epoch 234/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.2369 - accuracy: 1.0000 - val_loss: 58.6100 - val_accuracy: 1.0000\n",
            "Epoch 235/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 61.4868 - accuracy: 1.0000 - val_loss: 66.0425 - val_accuracy: 1.0000\n",
            "Epoch 236/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.5479 - accuracy: 1.0000 - val_loss: 59.5753 - val_accuracy: 1.0000\n",
            "Epoch 237/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 61.9553 - accuracy: 1.0000 - val_loss: 66.4184 - val_accuracy: 1.0000\n",
            "Epoch 238/300\n",
            "4435/4435 [==============================] - 19s 4ms/step - loss: 61.1703 - accuracy: 1.0000 - val_loss: 56.4752 - val_accuracy: 1.0000\n",
            "Epoch 239/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.7609 - accuracy: 1.0000 - val_loss: 56.9631 - val_accuracy: 1.0000\n",
            "Epoch 240/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 61.4936 - accuracy: 1.0000 - val_loss: 65.5242 - val_accuracy: 1.0000\n",
            "Epoch 241/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.6447 - accuracy: 1.0000 - val_loss: 75.2434 - val_accuracy: 1.0000\n",
            "Epoch 242/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 62.0468 - accuracy: 1.0000 - val_loss: 65.2244 - val_accuracy: 1.0000\n",
            "Epoch 243/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 60.7405 - accuracy: 1.0000 - val_loss: 74.9077 - val_accuracy: 1.0000\n",
            "Epoch 244/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.4687 - accuracy: 1.0000 - val_loss: 59.5938 - val_accuracy: 1.0000\n",
            "Epoch 245/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.5240 - accuracy: 1.0000 - val_loss: 73.6727 - val_accuracy: 1.0000\n",
            "Epoch 246/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.7729 - accuracy: 1.0000 - val_loss: 57.2970 - val_accuracy: 1.0000\n",
            "Epoch 247/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.1421 - accuracy: 1.0000 - val_loss: 74.9279 - val_accuracy: 1.0000\n",
            "Epoch 248/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.1878 - accuracy: 1.0000 - val_loss: 60.8114 - val_accuracy: 1.0000\n",
            "Epoch 249/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 61.4353 - accuracy: 1.0000 - val_loss: 58.0396 - val_accuracy: 1.0000\n",
            "Epoch 250/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.0186 - accuracy: 1.0000 - val_loss: 62.4897 - val_accuracy: 1.0000\n",
            "Epoch 251/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.5818 - accuracy: 1.0000 - val_loss: 62.4875 - val_accuracy: 1.0000\n",
            "Epoch 252/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 60.6759 - accuracy: 1.0000 - val_loss: 59.3754 - val_accuracy: 1.0000\n",
            "Epoch 253/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.1224 - accuracy: 1.0000 - val_loss: 58.3851 - val_accuracy: 1.0000\n",
            "Epoch 254/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.2717 - accuracy: 1.0000 - val_loss: 55.9225 - val_accuracy: 1.0000\n",
            "Epoch 255/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.0143 - accuracy: 1.0000 - val_loss: 56.3289 - val_accuracy: 1.0000\n",
            "Epoch 256/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.4100 - accuracy: 1.0000 - val_loss: 57.7034 - val_accuracy: 1.0000\n",
            "Epoch 257/300\n",
            "4435/4435 [==============================] - 19s 4ms/step - loss: 60.9659 - accuracy: 1.0000 - val_loss: 55.7464 - val_accuracy: 1.0000\n",
            "Epoch 258/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 61.0385 - accuracy: 1.0000 - val_loss: 60.9252 - val_accuracy: 1.0000\n",
            "Epoch 259/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.5027 - accuracy: 1.0000 - val_loss: 57.3174 - val_accuracy: 1.0000\n",
            "Epoch 260/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.4626 - accuracy: 1.0000 - val_loss: 61.4128 - val_accuracy: 1.0000\n",
            "Epoch 261/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.2597 - accuracy: 1.0000 - val_loss: 59.0281 - val_accuracy: 1.0000\n",
            "Epoch 262/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.2879 - accuracy: 1.0000 - val_loss: 60.2519 - val_accuracy: 1.0000\n",
            "Epoch 263/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.2686 - accuracy: 1.0000 - val_loss: 71.3375 - val_accuracy: 1.0000\n",
            "Epoch 264/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.5745 - accuracy: 1.0000 - val_loss: 56.9684 - val_accuracy: 1.0000\n",
            "Epoch 265/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.1625 - accuracy: 1.0000 - val_loss: 55.5413 - val_accuracy: 1.0000\n",
            "Epoch 266/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.4736 - accuracy: 1.0000 - val_loss: 55.4549 - val_accuracy: 1.0000\n",
            "Epoch 267/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 60.7804 - accuracy: 1.0000 - val_loss: 59.4320 - val_accuracy: 1.0000\n",
            "Epoch 268/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.1773 - accuracy: 1.0000 - val_loss: 84.0910 - val_accuracy: 1.0000\n",
            "Epoch 269/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 61.1948 - accuracy: 1.0000 - val_loss: 57.6082 - val_accuracy: 1.0000\n",
            "Epoch 270/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.5242 - accuracy: 1.0000 - val_loss: 61.2174 - val_accuracy: 1.0000\n",
            "Epoch 271/300\n",
            "4435/4435 [==============================] - 19s 4ms/step - loss: 60.7491 - accuracy: 1.0000 - val_loss: 58.2348 - val_accuracy: 1.0000\n",
            "Epoch 272/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.1492 - accuracy: 1.0000 - val_loss: 60.4845 - val_accuracy: 1.0000\n",
            "Epoch 273/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 60.7074 - accuracy: 1.0000 - val_loss: 58.6047 - val_accuracy: 1.0000\n",
            "Epoch 274/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.4135 - accuracy: 1.0000 - val_loss: 65.9544 - val_accuracy: 1.0000\n",
            "Epoch 275/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 60.8778 - accuracy: 1.0000 - val_loss: 63.1080 - val_accuracy: 1.0000\n",
            "Epoch 276/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.8350 - accuracy: 1.0000 - val_loss: 61.5028 - val_accuracy: 1.0000\n",
            "Epoch 277/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.0925 - accuracy: 1.0000 - val_loss: 66.0224 - val_accuracy: 1.0000\n",
            "Epoch 278/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.7510 - accuracy: 1.0000 - val_loss: 55.9306 - val_accuracy: 1.0000\n",
            "Epoch 279/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.7346 - accuracy: 1.0000 - val_loss: 57.9906 - val_accuracy: 1.0000\n",
            "Epoch 280/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.6458 - accuracy: 1.0000 - val_loss: 63.2884 - val_accuracy: 1.0000\n",
            "Epoch 281/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 60.6553 - accuracy: 1.0000 - val_loss: 70.7839 - val_accuracy: 1.0000\n",
            "Epoch 282/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 60.5537 - accuracy: 1.0000 - val_loss: 54.3803 - val_accuracy: 1.0000\n",
            "Epoch 283/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 60.8422 - accuracy: 1.0000 - val_loss: 74.6559 - val_accuracy: 1.0000\n",
            "Epoch 284/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.5889 - accuracy: 1.0000 - val_loss: 60.5474 - val_accuracy: 1.0000\n",
            "Epoch 285/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 60.7866 - accuracy: 1.0000 - val_loss: 58.1755 - val_accuracy: 1.0000\n",
            "Epoch 286/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.8103 - accuracy: 1.0000 - val_loss: 59.5725 - val_accuracy: 1.0000\n",
            "Epoch 287/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.5354 - accuracy: 1.0000 - val_loss: 59.4287 - val_accuracy: 1.0000\n",
            "Epoch 288/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.4553 - accuracy: 1.0000 - val_loss: 79.3309 - val_accuracy: 1.0000\n",
            "Epoch 289/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.9472 - accuracy: 1.0000 - val_loss: 58.8594 - val_accuracy: 1.0000\n",
            "Epoch 290/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.3003 - accuracy: 1.0000 - val_loss: 61.1111 - val_accuracy: 1.0000\n",
            "Epoch 291/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 60.9219 - accuracy: 1.0000 - val_loss: 58.4242 - val_accuracy: 1.0000\n",
            "Epoch 292/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.6955 - accuracy: 1.0000 - val_loss: 55.0252 - val_accuracy: 1.0000\n",
            "Epoch 293/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 60.9252 - accuracy: 1.0000 - val_loss: 62.4986 - val_accuracy: 1.0000\n",
            "Epoch 294/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.4753 - accuracy: 1.0000 - val_loss: 56.5212 - val_accuracy: 1.0000\n",
            "Epoch 295/300\n",
            "4435/4435 [==============================] - 15s 3ms/step - loss: 60.6220 - accuracy: 1.0000 - val_loss: 64.1092 - val_accuracy: 1.0000\n",
            "Epoch 296/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 61.3094 - accuracy: 1.0000 - val_loss: 63.7597 - val_accuracy: 1.0000\n",
            "Epoch 297/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 60.5689 - accuracy: 1.0000 - val_loss: 57.6867 - val_accuracy: 1.0000\n",
            "Epoch 298/300\n",
            "4435/4435 [==============================] - 18s 4ms/step - loss: 61.0942 - accuracy: 1.0000 - val_loss: 57.1081 - val_accuracy: 1.0000\n",
            "Epoch 299/300\n",
            "4435/4435 [==============================] - 16s 4ms/step - loss: 61.0501 - accuracy: 1.0000 - val_loss: 55.7352 - val_accuracy: 1.0000\n",
            "Epoch 300/300\n",
            "4435/4435 [==============================] - 17s 4ms/step - loss: 60.3965 - accuracy: 1.0000 - val_loss: 67.4975 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ss_agent.show_results()"
      ],
      "metadata": {
        "id": "cvwH3n0Fzygy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "708880f3-052a-4966-91c5-353f9f024160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_names = [\n",
        "  'Ally_1_row', 'Ally_1_col',\n",
        "  'Ally_2_row', 'Ally_2_col',\n",
        "  'Ally_3_row', 'Ally_3_col',\n",
        "  'Ally_4_row', 'Ally_4_col',\n",
        "  'Ally_5_row', 'Ally_5_col',\n",
        "  'Adv_1_row', 'Adv_1_col',\n",
        "  'Adv_2_row', 'Adv_2_col',\n",
        "  'Adv_3_row', 'Adv_3_col',\n",
        "  'Adv_4_row', 'Adv_4_col',\n",
        "  'Adv_5_row', 'Adv_5_col',\n",
        "  'Adv_6_row', 'Adv_6_col',\n",
        "  'Target_row', 'Target_col',\n",
        "  'ini_row', 'ini_col'\n",
        "]\n",
        "output_names = ['V', 'W', 'Theta']\n",
        "\n",
        "expected_output = validation_df.loc[:, output_names]\n",
        "validation_inputs = validation_df.loc[:, input_names]\n",
        "\n",
        "test_metrics = ss_agent.model.evaluate(validation_inputs, expected_output)\n",
        "print('Test loss:', test_metrics[0])\n",
        "print('Test accuracy:', test_metrics[1])"
      ],
      "metadata": {
        "id": "5uWIb5mLNva2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebcbbc9-5891-410d-ecb3-af07e27ff8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1733/1733 [==============================] - 3s 2ms/step - loss: 67.4976 - accuracy: 1.0000\n",
            "Test loss: 67.49756622314453\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "ss_agent.save()\n",
        "os.system('zip -r results.zip results')\n",
        "files.download(\"results.zip\")"
      ],
      "metadata": {
        "id": "Ad039L-T29xM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1b332eec-f37e-4645-a5b4-423d1c569687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9678ad78-acdf-491d-8dd4-b7670f311e45\", \"results.zip\", 379146)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}